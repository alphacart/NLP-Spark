{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('twitter').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in data\n",
    "# from pyspark import SparkFiles\n",
    "# url =\"https://s3.amazonaws.com/zepl-trilogy-test/yelp_reviews_quarter.csv\"\n",
    "# spark.sparkContext.addFile(url)\n",
    "# df = spark.read.csv(SparkFiles.get(\"yelp_reviews_quarter.csv\"), sep=\",\", header=True)\n",
    "# df = df.withColumn('original', df['class'])\n",
    "# df = df.withColumn('tweet', df['text'])\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|                text|sentiment|original|\n",
      "+--------------------+---------+--------+\n",
      "|Why am I still ho...|        0|negative|\n",
      "|auch my wrist! it...|        0|negative|\n",
      "|stoped broadcasti...|        0|negative|\n",
      "|@Iyarchuleta I'm ...|        4|positive|\n",
      "|Oh My Word. It sl...|        4|positive|\n",
      "|@leemtwittin welc...|        4|positive|\n",
      "|in office...learn...|        0|negative|\n",
      "|@djdsf really? oh...|        0|negative|\n",
      "|    Watching up  lol|        4|positive|\n",
      "|@jakebells I don'...|        0|negative|\n",
      "|After waiting nea...|        4|positive|\n",
      "|@ep0pe86 u suppos...|        0|negative|\n",
      "|Watching 7 Pounds...|        0|negative|\n",
      "|lost my camera la...|        0|negative|\n",
      "|@LVM5 morning - n...|        0|negative|\n",
      "|I hope I don't sh...|        0|negative|\n",
      "|Oh god help me!!!...|        0|negative|\n",
      "|@snarkattack kick...|        4|positive|\n",
      "|@BlueSpirit3 i di...|        0|negative|\n",
      "|@theboatissinkin ...|        0|negative|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url =\"CSV_cleaned/tweets_sample2.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"tweets_sample2.csv\"), sep=\",\", header=True)\n",
    "df.show()\n",
    "\n",
    "# url =\"CSV_cleaned/tweets_sample.csv\"\n",
    "# spark.sparkContext.addFile(url)\n",
    "# df = spark.read.csv(SparkFiles.get(\"tweets_sample.csv\"), sep=\",\", header=True)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+------+\n",
      "|                text|sentiment|original|length|\n",
      "+--------------------+---------+--------+------+\n",
      "|Why am I still ho...|        0|negative|    57|\n",
      "|auch my wrist! it...|        0|negative|    37|\n",
      "|stoped broadcasti...|        0|negative|    94|\n",
      "|@Iyarchuleta I'm ...|        4|positive|    56|\n",
      "|Oh My Word. It sl...|        4|positive|   106|\n",
      "|@leemtwittin welc...|        4|positive|    39|\n",
      "|in office...learn...|        0|negative|    30|\n",
      "|@djdsf really? oh...|        0|negative|   138|\n",
      "|    Watching up  lol|        4|positive|    16|\n",
      "|@jakebells I don'...|        0|negative|    24|\n",
      "|After waiting nea...|        4|positive|   114|\n",
      "|@ep0pe86 u suppos...|        0|negative|    39|\n",
      "|Watching 7 Pounds...|        0|negative|    29|\n",
      "|lost my camera la...|        0|negative|    61|\n",
      "|@LVM5 morning - n...|        0|negative|   127|\n",
      "|I hope I don't sh...|        0|negative|    48|\n",
      "|Oh god help me!!!...|        0|negative|    79|\n",
      "|@snarkattack kick...|        4|positive|    76|\n",
      "|@BlueSpirit3 i di...|        0|negative|   111|\n",
      "|@theboatissinkin ...|        0|negative|    80|\n",
      "+--------------------+---------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a length column to be used as a future feature \n",
    "from pyspark.sql.functions import length\n",
    "# data = df.withColumn('length', length(df['tweet']))\n",
    "data = df.withColumn('length', length(df['text']))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "\n",
    "# Create all the features to the data set\n",
    "# pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "pos_neg_to_num = StringIndexer(inputCol='original',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "# tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num,tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data)\n",
    "cleaned = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|sentiment|original|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|\n",
      "+--------------------+---------+--------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Why am I still ho...|        0|negative|    57|  0.0|[why, am, i, stil...|[still, hopeful, ...|(262144,[36200,81...|(262144,[36200,81...|(262145,[36200,81...|\n",
      "|auch my wrist! it...|        0|negative|    37|  0.0|[auch, my, wrist!...|[auch, wrist!, hu...|(262144,[8679,265...|(262144,[8679,265...|(262145,[8679,265...|\n",
      "|stoped broadcasti...|        0|negative|    94|  0.0|[stoped, broadcas...|[stoped, broadcas...|(262144,[65844,73...|(262144,[65844,73...|(262145,[65844,73...|\n",
      "|@Iyarchuleta I'm ...|        4|positive|    56|  1.0|[@iyarchuleta, i'...|[@iyarchuleta, go...|(262144,[62166,11...|(262144,[62166,11...|(262145,[62166,11...|\n",
      "|Oh My Word. It sl...|        4|positive|   106|  1.0|[oh, my, word., i...|[oh, word., slipp...|(262144,[105551,1...|(262144,[105551,1...|(262145,[105551,1...|\n",
      "|@leemtwittin welc...|        4|positive|    39|  1.0|[@leemtwittin, we...|[@leemtwittin, we...|(262144,[30448,81...|(262144,[30448,81...|(262145,[30448,81...|\n",
      "|in office...learn...|        0|negative|    30|  0.0|[in, office...lea...|[office...learng,...|(262144,[135105,1...|(262144,[135105,1...|(262145,[135105,1...|\n",
      "|@djdsf really? oh...|        0|negative|   138|  0.0|[@djdsf, really?,...|[@djdsf, really?,...|(262144,[5381,912...|(262144,[5381,912...|(262145,[5381,912...|\n",
      "|    Watching up  lol|        4|positive|    16|  1.0|[watching, up, , ...|   [watching, , lol]|(262144,[31950,63...|(262144,[31950,63...|(262145,[31950,63...|\n",
      "|@jakebells I don'...|        0|negative|    24|  0.0|[@jakebells, i, d...|  [@jakebells, know]|(262144,[568,1409...|(262144,[568,1409...|(262145,[568,1409...|\n",
      "|After waiting nea...|        4|positive|   114|  1.0|[after, waiting, ...|[waiting, nearly,...|(262144,[4054,326...|(262144,[4054,326...|(262145,[4054,326...|\n",
      "|@ep0pe86 u suppos...|        0|negative|    39|  0.0|[@ep0pe86, u, sup...|[@ep0pe86, u, sup...|(262144,[92424,99...|(262144,[92424,99...|(262145,[92424,99...|\n",
      "|Watching 7 Pounds...|        0|negative|    29|  0.0|[watching, 7, pou...|[watching, 7, pou...|(262144,[63139,77...|(262144,[63139,77...|(262145,[63139,77...|\n",
      "|lost my camera la...|        0|negative|    61|  0.0|[lost, my, camera...|[lost, camera, la...|(262144,[5381,191...|(262144,[5381,191...|(262145,[5381,191...|\n",
      "|@LVM5 morning - n...|        0|negative|   127|  0.0|[@lvm5, morning, ...|[@lvm5, morning, ...|(262144,[10562,11...|(262144,[10562,11...|(262145,[10562,11...|\n",
      "|I hope I don't sh...|        0|negative|    48|  0.0|[i, hope, i, don'...|[hope, show, late...|(262144,[34343,68...|(262144,[34343,68...|(262145,[34343,68...|\n",
      "|Oh god help me!!!...|        0|negative|    79|  0.0|[oh, god, help, m...|[oh, god, help, m...|(262144,[57304,88...|(262144,[57304,88...|(262145,[57304,88...|\n",
      "|@snarkattack kick...|        4|positive|    76|  1.0|[@snarkattack, ki...|[@snarkattack, ki...|(262144,[47767,52...|(262144,[47767,52...|(262145,[47767,52...|\n",
      "|@BlueSpirit3 i di...|        0|negative|   111|  0.0|[@bluespirit3, i,...|[@bluespirit3, om...|(262144,[70376,91...|(262144,[70376,91...|(262145,[70376,91...|\n",
      "|@theboatissinkin ...|        0|negative|    80|  0.0|[@theboatissinkin...|[@theboatissinkin...|(262144,[53261,64...|(262144,[53261,64...|(262145,[53261,64...|\n",
      "+--------------------+---------+--------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(262145,[36200,81...|\n",
      "|  0.0|(262145,[8679,265...|\n",
      "|  0.0|(262145,[65844,73...|\n",
      "|  1.0|(262145,[62166,11...|\n",
      "|  1.0|(262145,[105551,1...|\n",
      "|  1.0|(262145,[30448,81...|\n",
      "|  0.0|(262145,[135105,1...|\n",
      "|  0.0|(262145,[5381,912...|\n",
      "|  1.0|(262145,[31950,63...|\n",
      "|  0.0|(262145,[568,1409...|\n",
      "|  1.0|(262145,[4054,326...|\n",
      "|  0.0|(262145,[92424,99...|\n",
      "|  0.0|(262145,[63139,77...|\n",
      "|  0.0|(262145,[5381,191...|\n",
      "|  0.0|(262145,[10562,11...|\n",
      "|  0.0|(262145,[34343,68...|\n",
      "|  0.0|(262145,[57304,88...|\n",
      "|  1.0|(262145,[47767,52...|\n",
      "|  0.0|(262145,[70376,91...|\n",
      "|  0.0|(262145,[53261,64...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show label of ham spame and resulting features\n",
    "cleaned = cleaned.select(['label', 'features'])\n",
    "cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(262145,[14,925,2...|[-953.91754589477...|[0.99999893448254...|       0.0|\n",
      "|  0.0|(262145,[14,2437,...|[-792.39284337774...|[1.95020127735542...|       1.0|\n",
      "|  0.0|(262145,[14,4200,...|[-615.56084582777...|[0.08890824553060...|       1.0|\n",
      "|  0.0|(262145,[14,5381,...|[-910.17245083104...|[1.0,1.1113091698...|       0.0|\n",
      "|  0.0|(262145,[14,6981,...|[-1270.0105351853...|[1.67904095239364...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tranform the model with the testing data\n",
    "test_results = predictor.transform(testing)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.630461761049206\n"
     ]
    }
   ],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(f\"Accuracy of model at predicting reviews was: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
