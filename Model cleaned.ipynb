{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('twitter').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+-------+--------+--------+---------+\n",
      "|               tweet|Compound|Negative|Neutral|Positive|original|sentiment|\n",
      "+--------------------+--------+--------+-------+--------+--------+---------+\n",
      "|Happy Monday twit...|   0.778|   0.564|   0.09|   0.346|positive|        4|\n",
      "|C'MON STEWARDESS!...|     0.0|     1.0|    0.0|     0.0|negative|        0|\n",
      "|@saraeden That so...|   0.806|   0.452|    0.0|   0.548|positive|        4|\n",
      "|@mitchelmusso I R...|   0.684|   0.771|    0.0|   0.229|negative|        0|\n",
      "|I don't know what...|     0.0|     1.0|    0.0|     0.0|negative|        0|\n",
      "|Changeling. So fa...|    0.03|   0.369|   0.31|   0.321|negative|        0|\n",
      "|@kateadams will t...|     0.0|     1.0|    0.0|     0.0|positive|        4|\n",
      "|cuddling with mys...|     0.0|     1.0|    0.0|     0.0|negative|        0|\n",
      "|@mcttron Rip it u...|     0.0|     1.0|    0.0|     0.0|positive|        4|\n",
      "|@eatsomemore hell...|     0.0|     1.0|    0.0|     0.0|positive|        4|\n",
      "| I cant tomorrow ...|     0.0|     1.0|    0.0|     0.0|negative|        0|\n",
      "|@jesscorrie aww  ...|   0.836|    0.44|    0.0|    0.56|positive|        4|\n",
      "|@Daddys_pet yes i...|   0.796|   0.426|    0.0|   0.574|positive|        4|\n",
      "|@ClarenceHill You...|     0.0|     1.0|    0.0|     0.0|negative|        0|\n",
      "|Opps what i meant...|   0.667|   0.817|    0.0|   0.183|positive|        4|\n",
      "|@JoannaHang I was...|  -0.585|   0.678|  0.322|     0.0|negative|        0|\n",
      "|gonna get some Be...|    0.66|   0.649|    0.0|   0.351|positive|        4|\n",
      "|@theskorpion why ...|     0.0|     1.0|    0.0|     0.0|negative|        0|\n",
      "|@l3xi3sunshine ar...|  -0.494|    0.61|   0.39|     0.0|negative|        0|\n",
      "|eyes are so sore ...|  -0.775|   0.632|  0.317|   0.051|negative|        0|\n",
      "+--------------------+--------+--------+-------+--------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url =\"CSV_cleaned/tweets_sample2.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"tweets_sample2.csv\"), sep=\",\", header=True)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+-------+--------+--------+---------+------+\n",
      "|               tweet|Compound|Negative|Neutral|Positive|original|sentiment|length|\n",
      "+--------------------+--------+--------+-------+--------+--------+---------+------+\n",
      "|Happy Monday twit...|   0.778|   0.564|   0.09|   0.346|positive|        4|   116|\n",
      "|C'MON STEWARDESS!...|     0.0|     1.0|    0.0|     0.0|negative|        0|   136|\n",
      "|@saraeden That so...|   0.806|   0.452|    0.0|   0.548|positive|        4|    54|\n",
      "|@mitchelmusso I R...|   0.684|   0.771|    0.0|   0.229|negative|        0|   132|\n",
      "|I don't know what...|     0.0|     1.0|    0.0|     0.0|negative|        0|    27|\n",
      "|Changeling. So fa...|    0.03|   0.369|   0.31|   0.321|negative|        0|    38|\n",
      "|@kateadams will t...|     0.0|     1.0|    0.0|     0.0|positive|        4|    35|\n",
      "|cuddling with mys...|     0.0|     1.0|    0.0|     0.0|negative|        0|    21|\n",
      "|@mcttron Rip it u...|     0.0|     1.0|    0.0|     0.0|positive|        4|    67|\n",
      "|@eatsomemore hell...|     0.0|     1.0|    0.0|     0.0|positive|        4|    43|\n",
      "| I cant tomorrow ...|     0.0|     1.0|    0.0|     0.0|negative|        0|    32|\n",
      "|@jesscorrie aww  ...|   0.836|    0.44|    0.0|    0.56|positive|        4|    67|\n",
      "|@Daddys_pet yes i...|   0.796|   0.426|    0.0|   0.574|positive|        4|    52|\n",
      "|@ClarenceHill You...|     0.0|     1.0|    0.0|     0.0|negative|        0|    41|\n",
      "|Opps what i meant...|   0.667|   0.817|    0.0|   0.183|positive|        4|   119|\n",
      "|@JoannaHang I was...|  -0.585|   0.678|  0.322|     0.0|negative|        0|    58|\n",
      "|gonna get some Be...|    0.66|   0.649|    0.0|   0.351|positive|        4|    58|\n",
      "|@theskorpion why ...|     0.0|     1.0|    0.0|     0.0|negative|        0|    51|\n",
      "|@l3xi3sunshine ar...|  -0.494|    0.61|   0.39|     0.0|negative|        0|    33|\n",
      "|eyes are so sore ...|  -0.775|   0.632|  0.317|   0.051|negative|        0|   119|\n",
      "+--------------------+--------+--------+-------+--------+--------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a length column to be used as a future feature\n",
    "data = df.withColumn('length', length(df['tweet']))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the features to the data set\n",
    "\n",
    "pos_neg_to_num = StringIndexer(inputCol='original',outputCol='label')\n",
    "pos_neg_to_num2 = StringIndexer(inputCol='Compound',outputCol='compound2')\n",
    "pos_neg_to_num3 = StringIndexer(inputCol='Positive',outputCol='positive2')\n",
    "pos_neg_to_num4 = StringIndexer(inputCol='Negative',outputCol='negative2')\n",
    "pos_neg_to_num5 = StringIndexer(inputCol='Neutral',outputCol='neutral2')\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length','compound2','negative2','positive2','neutral2'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num,pos_neg_to_num2,pos_neg_to_num3,pos_neg_to_num4,pos_neg_to_num5,tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data)\n",
    "cleaned = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+\n",
      "|               tweet|           idf_token|length|compound2|negative2|positive2|neutral2|label|            features|\n",
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+\n",
      "|Happy Monday twit...|(262144,[21872,37...|   116|     43.0|    167.0|    139.0|    82.0|  1.0|(262149,[21872,37...|\n",
      "|C'MON STEWARDESS!...|(262144,[304,3091...|   136|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[304,3091...|\n",
      "|@saraeden That so...|(262144,[113432,1...|    54|    610.0|    614.0|    403.0|     0.0|  1.0|(262149,[113432,1...|\n",
      "|@mitchelmusso I R...|(262144,[14,33053...|   132|    151.0|    331.0|    155.0|     0.0|  0.0|(262149,[14,33053...|\n",
      "|I don't know what...|(262144,[140931,2...|    27|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[140931,2...|\n",
      "|Changeling. So fa...|(262144,[155321,1...|    38|    669.0|    583.0|    461.0|    38.0|  0.0|(262149,[155321,1...|\n",
      "|@kateadams will t...|(262144,[88244,15...|    35|      0.0|      0.0|      0.0|     0.0|  1.0|(262149,[88244,15...|\n",
      "|cuddling with mys...|(262144,[38629],[...|    21|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[38629,26...|\n",
      "|@mcttron Rip it u...|(262144,[5381,266...|    67|      0.0|      0.0|      0.0|     0.0|  1.0|(262149,[5381,266...|\n",
      "|@eatsomemore hell...|(262144,[19140,28...|    43|      0.0|      0.0|      0.0|     0.0|  1.0|(262149,[19140,28...|\n",
      "| I cant tomorrow ...|(262144,[29129,12...|    32|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[29129,12...|\n",
      "|@jesscorrie aww  ...|(262144,[55724,82...|    67|    200.0|    446.0|    350.0|     0.0|  1.0|(262149,[55724,82...|\n",
      "|@Daddys_pet yes i...|(262144,[39276,54...|    52|     65.0|     32.0|     62.0|     0.0|  1.0|(262149,[39276,54...|\n",
      "|@ClarenceHill You...|(262144,[87348,15...|    41|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[87348,15...|\n",
      "|Opps what i meant...|(262144,[2054,844...|   119|    377.0|    112.0|     51.0|     0.0|  1.0|(262149,[2054,844...|\n",
      "|@JoannaHang I was...|(262144,[16143,57...|    58|     66.0|    111.0|      0.0|   137.0|  0.0|(262149,[16143,57...|\n",
      "|gonna get some Be...|(262144,[99895,12...|    58|     47.0|     42.0|    123.0|     0.0|  1.0|(262149,[99895,12...|\n",
      "|@theskorpion why ...|(262144,[24150,11...|    51|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[24150,11...|\n",
      "|@l3xi3sunshine ar...|(262144,[113318,1...|    33|     78.0|    186.0|      0.0|   267.0|  0.0|(262149,[113318,1...|\n",
      "|eyes are so sore ...|(262144,[8449,219...|   119|    714.0|     39.0|    510.0|   174.0|  0.0|(262149,[8449,219...|\n",
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaned.show()\n",
    "\n",
    "# cleaned.select(['label','stop_tokens', 'features']).show()\n",
    "\n",
    "cleaned= cleaned.select(['tweet','idf_token', 'length','compound2','negative2','positive2','neutral2','label', 'features'])\n",
    "cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|               tweet|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "| OMG! i hear ever...|       1.0|[0.00165477759632...|\n",
      "| youtwitface  #yo...|       1.0|[0.12865392665973...|\n",
      "|#jaredleto he loo...|       1.0|[7.86315677143086...|\n",
      "|#musicmonday Brit...|       1.0|[1.80540529830780...|\n",
      "|#squarespace my f...|       1.0|[6.97034232321681...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tranform the model with the testing data\n",
    "test_results = predictor.transform(testing)\n",
    "# test_results.show(5)\n",
    "\n",
    "\n",
    "test_results.select(['tweet','prediction', 'probability']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|               tweet|           idf_token|length|compound2|negative2|positive2|neutral2|label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+--------------------+--------------------+----------+\n",
      "| OMG! i hear ever...|(262144,[37101,10...|    47|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[37101,10...|[-523.79292034120...|[0.00165477759632...|       1.0|\n",
      "| youtwitface  #yo...|(262144,[167348,1...|    39|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[167348,1...|[-411.51875849300...|[0.12865392665973...|       1.0|\n",
      "|#jaredleto he loo...|(262144,[27582,29...|   134|    402.0|    263.0|    260.0|     0.0|  1.0|(262149,[27582,29...|[-3416.0056328282...|[7.86315677143086...|       1.0|\n",
      "|#musicmonday Brit...|(262144,[16426,67...|    99|     17.0|     74.0|     80.0|     0.0|  1.0|(262149,[16426,67...|[-1140.4606740326...|[1.80540529830780...|       1.0|\n",
      "|#squarespace my f...|(262144,[329,3620...|    82|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[329,3620...|[-682.51980049275...|[6.97034232321681...|       1.0|\n",
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.6704865512565825\n"
     ]
    }
   ],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(f\"Accuracy of model at predicting reviews was: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_up = VectorAssembler(inputCols=['idf_token', 'length','compound2'], outputCol='features') 0.5919176454727069\n",
    " \n",
    "# clean_up = VectorAssembler(inputCols=['idf_token', 'compound2'], outputCol='features') 0.5707940034594462\n",
    "\n",
    "# clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features') 0.6008312043512044\n",
    "\n",
    "# clean_up = VectorAssembler(inputCols=['idf_token', 'length','compound2','negative2','positive2','neutral2'], outputCol='features') 0.6785473212243401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o480.save.\n: java.io.IOException: Path sentiment_model.h5 already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:503)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:102)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0ecfe8efe7f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Save Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/PythonData/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o480.save.\n: java.io.IOException: Path sentiment_model.h5 already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:503)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:102)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "##Save Model\n",
    "predictor.save(\"sentiment_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Load model\n",
    "new_predictor = NaiveBayesModel.load(\"sentiment_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|               tweet|           idf_token|length|compound2|negative2|positive2|neutral2|label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+--------------------+--------------------+----------+\n",
      "| OMG! i hear ever...|(262144,[37101,10...|    47|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[37101,10...|[-485.37778564159...|[0.99999997583999...|       0.0|\n",
      "| youtwitface  #yo...|(262144,[167348,1...|    39|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[167348,1...|[-411.16899338807...|[0.15216455109871...|       1.0|\n",
      "|#jaredleto he loo...|(262144,[27582,29...|   134|    402.0|    263.0|    260.0|     0.0|  1.0|(262149,[27582,29...|[-3420.5557424115...|[3.35944754728711...|       1.0|\n",
      "|#musicmonday Brit...|(262144,[16426,67...|    99|     17.0|     74.0|     80.0|     0.0|  1.0|(262149,[16426,67...|[-1136.3894768048...|[6.71368831888531...|       1.0|\n",
      "|#squarespace my f...|(262144,[329,3620...|    82|      0.0|      0.0|      0.0|     0.0|  0.0|(262149,[329,3620...|[-634.09977717909...|[0.99999999999999...|       0.0|\n",
      "+--------------------+--------------------+------+---------+---------+---------+--------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = new_predictor.transform(testing)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.8243621616156596\n"
     ]
    }
   ],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(f\"Accuracy of model at predicting reviews was: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+-------+--------+---+----+------+\n",
      "|               tweet|Compound|Negative|Neutral|Positive|lat|long|length|\n",
      "+--------------------+--------+--------+-------+--------+---+----+------+\n",
      "|Happy Monday twitter|   0.778|   0.564|   0.09|   0.346|100| -90|    20|\n",
      "+--------------------+--------+--------+-------+--------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testdata= spark.createDataFrame([    \n",
    "#     (\"Happy Monday twitter\", 0.778,0.564,0.09,0.346)\n",
    "# ], [\"tweet\", \"Compound\",\"Negative\",\"Neutral\",\"Positive\"])\n",
    "\n",
    "testdata2= spark.createDataFrame([    \n",
    "    (\"Happy Monday twitter\", 0.778,0.564,0.09,0.346, 100,-90)\n",
    "], [\"tweet\", \"Compound\",\"Negative\",\"Neutral\",\"Positive\", \"lat\",\"long\"])\n",
    "\n",
    "data = testdata2.withColumn('length', length(testdata2['tweet']))\n",
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the features to the data set\n",
    "pos_neg_to_num2 = StringIndexer(inputCol='Compound',outputCol='compound2')\n",
    "pos_neg_to_num3 = StringIndexer(inputCol='Positive',outputCol='positive2')\n",
    "pos_neg_to_num4 = StringIndexer(inputCol='Negative',outputCol='negative2')\n",
    "pos_neg_to_num5 = StringIndexer(inputCol='Neutral',outputCol='neutral2')\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')\n",
    "\n",
    "\n",
    "# Create feature vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length','compound2','negative2','positive2','neutral2'], outputCol='features')\n",
    "\n",
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num2,pos_neg_to_num3,pos_neg_to_num4,pos_neg_to_num5,tokenizer, stopremove, hashingTF, idf, clean_up])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+-------+--------+---+----+------+---------+---------+---------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               tweet|Compound|Negative|Neutral|Positive|lat|long|length|compound2|positive2|negative2|neutral2|          token_text|         stop_tokens|          hash_token|           idf_token|            features|\n",
      "+--------------------+--------+--------+-------+--------+---+----+------+---------+---------+---------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Happy Monday twitter|   0.778|   0.564|   0.09|   0.346|100| -90|    20|      0.0|      0.0|      0.0|     0.0|[happy, monday, t...|[happy, monday, t...|(262144,[64274,86...|(262144,[64274,86...|(262149,[262144],...|\n",
      "+--------------------+--------+--------+-------+--------+---+----+------+---------+---------+---------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data)\n",
    "cleaned = cleaner.transform(data)\n",
    "cleaned.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       1.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#apply model\n",
    "\n",
    "test_results = new_predictor.transform(cleaned)\n",
    "test_results.select([\"prediction\"]).show(5)\n",
    "\n",
    "\n",
    "# cleaned.select(['tweet','idf_token', 'length','compound2','negative2','positive2','neutral2','label', 'features'])\n",
    "# cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
